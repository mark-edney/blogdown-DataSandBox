df['Class'] -= 1
y = np.argmax(df[predictors].values, axis =1) + 1
X = df.drop(predictors, axis = 1)
sc = StnadardSCaler()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
y = np.argmax(df[predictors].values, axis =1) + 1
X = df.drop(predictors, axis = 1)
sc = StandardSCaler()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
sc = StandardScaler()
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
y = np.argmax(df[predictors].values, axis =1) + 1
X = df.drop(predictors, axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
sc = StandardScaler()
scaler = sc.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_test_sc = scaler.transform(X_test)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), (100,100,100,100), (100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train_sc, y_train)
grid.best_score
grid.best_score_
grid.best_estimator_
grid.classes_
grid.best_index_
grid.best_params_
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
y = np.argmax(df[predictors].values, axis =1)
X = df.drop(predictors, axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), (100,100,100,100), (100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train, y_train)
grid.best_score_
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
scaler = sc.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_test_sc = scaler.transform(X_test)
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), (100,100,100,100), (100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train_sc, y_train)
grid.best_score_
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), (100,100,100,100), (100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train, y_train)
grid.best_score_
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), (100,100,100,100), (100,100,100,100,100), (100,100,100,100,100,100), (100,100,100,100,100,100, 100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train, y_train)
grid.best_score_
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
scaler = sc.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_test_sc = scaler.transform(X_test)
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100), (100,100,100,100), (100,100,100,100,100), (100,100,100,100,100,100), (100,100,100,100,100,100, 100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train_sc, y_train)
grid.best_score_
grid.best_estimator_
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train, y_train)
grid.best_score_
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
y = np.argmax(df[predictors].values, axis =1)
X = df.drop(predictors, axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters)
grid.fit(X_train, y_train)
grid.best_score_
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
scaler = sc.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_test_sc = scaler.transform(X_test)
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train_sc, y_train)
grid.best_score_
grid.score(X_test_sc, y_test)
blogdown:::preview_site()
View(train_test_split)
quit
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = FALSE)
library(blogdown)
serve_site()
reticulate::repl_python()
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters)
grid.fit(X_train, y_train)
print(grid.best_score_)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters)
grid.fit(X_train, y_train)
print(grid.best_score_)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters)
grid.fit(X_train, y_train)
print(grid.best_score_)
quit
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = FALSE)
stop_server()
blogdown:::preview_site()
stop_server()
blogdown:::preview_site()
stop_server()
blogdown:::preview_site()
serve_site()
blogdown:::preview_site()
library(blogdown)
serve_site()
blogdown:::preview_site()
reticulate::repl_python()
```{python data}
```{python data}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters)
grid.fit(X_train, y_train)
print(grid.best_score_)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('https://www.openml.org/data/get_csv/1592296/php9xWOpn')
predictors = ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'Class']
df['Class'] -= 1
y = np.argmax(df[predictors].values, axis =1)
X = df.drop(predictors, axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters)
grid.fit(X_train, y_train)
print(grid.best_score_)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
scaler = sc.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_test_sc = scaler.transform(X_test)
parameters = {'hidden_layer_sizes':[(1),(100), (100,100), (100,100,100),
(100,100,100,100),
(100,100,100,100,100),
(100,100,100,100,100,100),
(100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100),
(100,100,100,100,100,100,100,100,100,100)]}
model = MLPClassifier(random_state = 1,max_iter = 10000,
solver = 'adam', learning_rate = 'adaptive')
grid = GridSearchCV(estimator = model, param_grid = parameters, cv=3)
grid.fit(X_train_sc, y_train)
grid.best_score_
grid.score(X_test_sc, y_test)
library(blogdown)
serve_site()
install.packages("appsilon")
blogdown:::new_post_addin()
install.packages("dlstats")
library(dlstats)
?dlstats
dlstats::bioc_stats()
devtools::install_github("metacran/cranlogs")
library(devtools)
install.packages("devtools")
devtools::install_github("metacran/cranlogs")
library(cranlogs)
library(cranlogs)
install.packages("cranlogs")
library(cranlogs)
remove.packages("cranlogs", lib="~/R/win-library/4.1")
blogdown:::preview_site()
library(cranlogs)
top100 <- cran_top_downloads(when = 'last-month', count = 100)
library(tidyverse)
library(tidyverse)
library(cranlogs)
top100 <- cran_top_downloads(when = 'last-month', count = 100)
top100 %>% head()
mine <- installed.packages()
mine
mine <- installed.packages() %>%
select(package)
mine <- installed.packages() %>%
data.frame() %>%
select(package)
mine <- installed.packages() %>%
data.frame()
View(mine)
mine <- installed.packages() %>%
data.frame() %>%
select(Package)
View(mine)
?cran_top_downloads
mine <- installed.packages() %>%
data.frame() %>%
select(Package)
top100 %>%
filter(!Package %in% mine$Package)
top100 %>%
filter(!package %in% mine$Package)
new <- top100 %>%
filter(!package %in% mine$Package)
View(new)
library(tidyverse)
library(cranlogs)
top100 <- cran_top_downloads(when = 'last-month', count = 100)
top100 %>% head()
?rlang
?glue
cli
?clip
?cli
mine <- installed.packages() %>%
data.frame() %>%
select(Package)
new <- top100 %>%
filter(!package %in% mine$Package)
new
new$package %>%
cran_downloads(when = "last-month")
new$package %>%
cran_downloads(when = "last-month") %>%
ggplot(aes(x = date, y = count, color = package)) +
geom_line()
View(new)
blogdown:::preview_site()
blogdown:::new_post_addin()
reticulate::repl_python()
import os
import cv2 as cv
from PIL import Image
from PIL import Image
img = Image.open(input())
from PIL import Image
import os, sys
from PIL import Image
import os, sys
from PIL import Image
```{python input}
#img = Image.open(input())
img = Image.open('Rlogo.svg')
import cairosvg
import os, sys
import cairosvg
```{python intro}
import cairosvg
import os, sys, cairosvg
import os, sys, cairosvg
import os, sys, cairosvg
import os, sys, svglib
from svglib.svglib import svg2rlg
from reportlab.graphics import renderPDF, renderPM
library(blogdown)
quit
library(blogdown)
blogdown::check_site()
blogdown::check_site()
blogdown::build_site()
blogdown::serve_site()
blogdown:::new_post_addin()
install.packages("data.table")
install.packages("farff")
df <- farff::readARFF('dataset_31_credit-g.arff')
View(df)
df
type(df)
str(ddf)
str(df)
readARFF
?readARFF
View(df)
library(tidyverse)
library(data.table)
library(microbenchmark)
library(farff)
setwd("~/R/blogdown-DataSandBox/content/post/2022-04-08-benchmarking-data-tables")
dt <- setdf(df)
df <- farff::readARFF('dataset_31_credit-g.arff')
dt <- setDT(df)
str(dt)
ti <- tibble(df)
str(ti)
blogdown:::preview_site()
library(tidyverse)
library(data.table)
library(microbenchmark)
library(farff)
View(df)
View(df)
microbenchmark(df %>% group_by(class) %>%
summarise(mean(age)))
df %>% group_by(class) %>%
summarise(mean(age))
df %>% group_by(class) %>%
summarise(mean(age)) %>% microbenchmark()
microbenchmark(df %>% group_by(class) %>%
summarise(mean(age)))
?microbenchmark
dt[,mean(age), by = class]
df %>% group_by(class) %>%
summarise(mean(age)
)
microbenchmark(df %>% group_by(class) %>%
summarise(avg = mean(age)),
dt[,avg = mean(age), by = class])
dt[,avg = mean(age), by = class]
dt[,.(avg = mean(age)), by = class]
df %>% group_by(class) %>%
summarise(avg = mean(age))
ti %>% group_by(class) %>%
summarise(avg = mean(age))
microbenchmark(df %>% group_by(class) %>%
summarise(avg = mean(age)),
dt[,.(avg = mean(age)), by = class],
ti %>% group_by(class) %>%
summarise(avg = mean(age)))
library(microbenchmark)
microbenchmark(sqrt(3),
4 |> sqrt(),
5 %>% sqrt())
result <- microbenchmark(df %>% group_by(class) %>%
summarise(avg = mean(age)),
dt[,.(avg = mean(age)), by = class],
ti %>% group_by(class) %>%
summarise(avg = mean(age)))
result$time
result$expr
result
View(result)
result %>% group_by(expr) %>% summarise(mean(time))
result %>%
group_by(expr) %>%
summarise(mean(time))
result <- microbenchmark(df %>%
group_by(class) %>%
summarise(avg = mean(age)),
dt[,.(avg = mean(age)), by = class],
ti %>%
group_by(class) %>%
summarise(avg = mean(age)))
result %>%
group_by(expr) %>%
summarise(mean(time))
result <- microbenchmark(df %>%
group_by(class) %>%
summarise(avg = mean(age)),
dt[,.(avg = mean(age)), by = class],
ti %>%
group_by(class) %>%
summarise(avg = mean(age)),
times = 1000L)
result %>%
group_by(expr) %>%
summarise(mean(time))
microbenchmark(df %>%
group_by(class) %>%
summarise(avg = mean(age)),
dt[,.(avg = mean(age)), by = class],
ti %>%
group_by(class) %>%
summarise(avg = mean(age)),
times = 1000L)
microbenchmark(df %>%
group_by(class) %>%
summarise(avg = mean(age)),
dt[,.(avg = mean(age)), by = class],
ti %>%
group_by(class) %>%
summarise(avg = mean(age)))
