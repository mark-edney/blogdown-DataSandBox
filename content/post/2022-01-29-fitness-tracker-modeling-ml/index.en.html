---
title: 'Fitness Tracker Modeling: ML'
author: Mark Edney
date: '2022-01-29'
categories:
  - Project
tags:
  - GGPlot
  - ML
  - R
draft: no
image: "img/MachineLearningProject.png"
archives:
  - 2022/01
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<blockquote>
<p>The original paper was written on 12/18/2020</p>
</blockquote>
<div id="executive-summary" class="section level1">
<h1>Executive Summary</h1>
<p>This report analyzes collected data on different users preforming barbell lifts
performed at different levels of quality. A machine learning algorithm was used
to create a model to determine the users rating based on data collected from multiple
accelerometers. More information on the project can be found <a href="http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har">here</a>.</p>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<div id="initialization" class="section level2">
<h2>Initialization</h2>
<p>The following code was used to initialize the required r libraries as well as downloading
the required data and store it into memory. There are some of the columns off the
data that were not required for modeling which were excluded (ex. user names).</p>
<pre class="r"><code>library(caret)
library(gbm)
library(dplyr)
library(randomForest)
library(ggplot2)
set.seed(90210)
Ntree &lt;- 200

download.file(&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;, &quot;training.csv&quot;)
download.file(&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;, &quot;testing.csv&quot;)
train &lt;- read.csv2(&quot;training.csv&quot;, sep = &quot;,&quot;)[,-c(1:7)]
test &lt;- read.csv2(&quot;testing.csv&quot;, sep = &quot;,&quot;)[,-c(1:7)]</code></pre>
</div>
<div id="reducing-predictors" class="section level2">
<h2>Reducing predictors</h2>
<p>The data contains way too many predictors (153 in total) to produce accurate and simple models.
Some trimming is required. The first trim is performed with the near zero variance function
from the caret library which finds the predictors that exhibit near zero variation.
These predictors would add little benefit to include in models.</p>
<pre class="r"><code>nz &lt;- nearZeroVar(train)
train &lt;- train[,-nz]
test &lt;- test[-nz]</code></pre>
<p>From this step, the number of predictors is reduced to 94. There remains
a large number of NA values in the data. These values are examined in the next chunk
of code.</p>
<pre class="r"><code>maxi &lt;- length(train) - 1
valna &lt;- 1:maxi

for (i in 1:maxi) {
        train[,i] &lt;- as.numeric(train[,i])
        test[,i] &lt;- as.numeric(test[,i])
        valna[i] &lt;- mean(is.na(train[,i]))
}

table(valna)</code></pre>
<pre><code>## valna
##                 0 0.979308938946081 
##                52                41</code></pre>
<p>The code shows that there are 52 predictors that have no missing
data and 41 predictors that are mostly missing values.
These predictors would add little value to the modeling and are removed with the
following code</p>
<pre class="r"><code>train &lt;- train[, valna == 0]
test &lt;- test[, valna == 0]</code></pre>
<p>The training was then divided to create a valdiation set which will be used for
cross validation. Note that the random forest algorithm has built in cross validation
with the “out of bag error”. About 1/3 of the data is used in a random forest.</p>
<pre class="r"><code>Valid &lt;- createDataPartition(train$classe, p = 0.3)[[1]]
valid &lt;- train[Valid,]
train &lt;- train[-Valid,]</code></pre>
<p>The next step is to utilize the variable of importance function in the caret library
to reduce the number of predictors even further. The train data is still very large
but by making a sample set from the training data and modeling from that we can get a
reasonable approximation of the variables of importance.</p>
<pre class="r"><code>strain &lt;- rbind(sample_n(train[train$classe == &quot;A&quot;,],round(mean(train$classe == &quot;A&quot;)*200,0)),
                sample_n(train[train$classe == &quot;B&quot;,],round(mean(train$classe == &quot;B&quot;)*200,0)),
                sample_n(train[train$classe == &quot;C&quot;,],round(mean(train$classe == &quot;C&quot;)*200,0)),
                sample_n(train[train$classe == &quot;D&quot;,],round(mean(train$classe == &quot;D&quot;)*200,0)),
                sample_n(train[train$classe == &quot;E&quot;,],round(mean(train$classe == &quot;E&quot;)*200,0))
)</code></pre>
<p>The sample set was set to ensure an accurate representation of the ‘classe’ variable
in the training data. Two models were completed and their variables of importance
were added together.</p>
<pre class="r"><code>mdl1 &lt;- train(classe~., data = strain, method = &quot;rf&quot;, ntree = Ntree)
mdl2 &lt;- train(classe~., data = strain, method = &quot;gbm&quot;, verbose = FALSE)
var &lt;- varImp(mdl1)$importance &gt; 50 | varImp(mdl2)$importance &gt; 50
varorder &lt;- order(varImp(mdl1)$importance, decreasing = TRUE)
Varimp &lt;- row.names(varImp(mdl1)$importance)[varorder[1:2]]</code></pre>
<p>A value of 50 was used for a cut-off value. The total number of predictors has been
reduced to 4.</p>
<pre class="r"><code>valid &lt;- valid[,var]
train &lt;- train[,var]
test &lt;- test[,var]</code></pre>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<p>With the reduced predictors, the models can now be trained. Since these model will look
at the entire training data set, it will require a lot of time. The models include:<br />
- Random forest<br />
- Generalized Boosted<br />
- Linear Discriminant<br />
- Combined<br />
The randomForest function is used as it is more efficient than the train function.
The data method is also more efficient then using the formula method.</p>
<pre class="r"><code>mdl11 &lt;- randomForest(x = train[,1:(ncol(train) - 1)], y = as.factor(train[,ncol(train)]), ntree = Ntree, proximity = TRUE)
mdl21 &lt;- train(classe~., data = train, method = &quot;gbm&quot;, verbose = FALSE)
mdl31 &lt;- train(classe~., data = train, method = &quot;lda&quot;)</code></pre>
<p>The following code constructs the combined model</p>
<pre class="r"><code>pmdl11 &lt;- predict(mdl11, valid)
pmdl21 &lt;- predict(mdl21, valid)
pmdl31 &lt;- predict(mdl31, valid)
join &lt;- data.frame(pmdl11, pmdl21, pmdl31, classe = valid$classe)
jmdl &lt;- randomForest(x = join[,1:3], y = as.factor(join$classe), ntree = Ntree)</code></pre>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model Evaluation</h2>
<p>The new models will need to be examined against the validation data set. The out of
bag error for the random forest models were not used as the validation data provides
a uniform comparison for all models. The following function was used to test the models:</p>
<pre class="r"><code>Exacc &lt;- function(mdl, test){
        mean(predict(mdl,test) == test$classe)
}</code></pre>
<p>The model’s accuracy are summarized in the following dataframe when they are used
to predict the results in the validation set:</p>
<pre><code>##   Model  accuracy
## 1 mdl11 0.8955680
## 2 mdl21 0.7994566
## 3 mdl31 0.3625403
## 4 joint 0.8920020</code></pre>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>From the results from the model testing, it is clear that the random forest and the
combined are the most accurate models for the validation set. The combined model
has approxiametly the same level of accuracy as the random forest meaning it is the most heavily
weighted model. It also means that the inclusion of the boosted and linear discriminant
models do not contribute to its accuracy. For simplification, the random forest
model is the best model.</p>
</div>
<div id="plot" class="section level1">
<h1>Plot</h1>
<p>The centers of the model can be found from the proximity data. The proximity data
is compared to two predictors, the most important predictors. The two most important
predictors would sometimes vary so the code was changed to be flexible to it.</p>
<pre class="r"><code>index &lt;- names(train) %in% Varimp
mdlp &lt;- classCenter(train[index], train$classe, mdl11$proximity)
mdlp &lt;- as.data.frame(mdlp)
mdlp$classe &lt;- rownames(mdlp)</code></pre>
<p>This centers data can be included with the training data. There are distinctly different
regions based off of the different classe values but the other predictors also contribute to
model accuracy.</p>
<pre class="r"><code>index &lt;- names(train) %in% Varimp
names &lt;- names(train[index])

f &lt;- function(name1, name2){
        xval &lt;- sym(name1)
        yval &lt;- sym(name2)
        ggplot(data = train, aes_string(x = xval, y = yval, col = &quot;classe&quot;)) +
                geom_point() +
                geom_point(aes_string(x = xval, y = yval, col = &quot;classe&quot;), size = 10, shape = 4, data = mdlp) +
                labs(title = &quot;Model centers on two variables of importance&quot;)
        }
f(names[1], names[2])</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/plot-1.png" width="672" /></p>
</div>
