---
title: Text Prediction App
author: Mark Edney
date: '2022-03-11'
slug: []
categories:
  - Project
tags:
  - Shiny App
  - NLP
  - R
draft: yes
description: 'A predictive Text Shiny Application'
image: "/img/predict_text.jpg"
archives:
  - 2022/03
latex: true
---

> This Shiny App was first written in May of 2021

## Description

The goal of this project was to create a N-gram based model to predict the word to follow the users input. This project was to complete the Capstone project for the Johns Hopkins University Data science program on coursera. The data for this project was provided by swiftkey.

## Initialization

The initial step that loads the required libraries and downloads the data sets if not all read on file.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = FALSE)
```

```{r initial}
library(tidyverse)
library(tidytext)
library(pryr)

#downloads the corpus files, profanity filter and English dictionary

url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
url4 <- "https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
       dir.create("~/R/Capestone/data/")}

if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
   file.exists("~/R/Capestone/data/prof.zip")==FALSE|
   file.exists("~/R/Capestone/data/diction.txt")==FALSE|
    file.exists("~/R/Capestone/data/contractions.txt")==FALSE){
        download.file(url,destfile = "~/R/Capestone/data/data.zip")
        download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
        download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
        download.file(url4,destfile = "~/R/Capestone/data/contractions.txt")
        setwd("~/R/Capestone/data/")
        unzip("~/R/Capestone/data/prof.zip")
        unzip("~/R/Capestone/data/data.zip")
        setwd("~/R/Capestone")
}
```

## Creating a Corpus

The project requires a Corpus, or a large body of text to create models. At this stage, the files are opened and joined. The Corpus is so large and requires some much ram that a sample of 10% is taken.

```{r corpus}

blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog) 
news <- tibble(text = news)
twitter <- tibble(text = twitter)

set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>% 
        slice_sample(prop = 0.10) %>%
        mutate(line = row_number())
```

## Corpus filtering

Here the corpus filter is created to remove profanity and any word that is not in the English dictionary.

```{r filter}
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)

english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])

contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract) %>% unnest_tokens(word,word)
```

## Vocabulary

A vocabulary of words is created from the unique words with the applied filters.

```{r vocab}
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)

unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
        semi_join(voc, by = c("ngram"="word")) 
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
```

## Out of Vocabulary

To model out of vocabulary words we take a sampling of the least frequent unigrams and change them to the character "<unk>". If a word is tested that isn't in the vocabulary for the corpus, the quantity will be converted to "<unk>".

```{r OOV}
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
        filter(n==1) %>%
        slice_sample(n = mean(unigramcount$n)) 
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "unk"

```

The Corpus can reconstructed with the reduced vocabulary remove the need to re-filter for the higher levels of n-grams.

```{r corpusremake}
corpus <- unigram %>%
        group_by(line) %>%
        summarise(line = paste(ngram, collapse = " ")) %>%
        rename("text" = "line") %>%
        mutate("line" = row_number())

rm(unigramcount)        
```

There are still many words that only appear once so it useful to remove these words as the add little to prediction value of the models but significantly to the time and memory requirement. This is mainly due to the filtering.

```{r vocabclean}
unigram <- unigram %>%
        count(ngram) %>%
        filter(n > quantile(n,0.4))

voc <- tibble(word = unigram$ngram)

```

## Bigrams

The bigrams are created and then split into individual words which can be filtered by the vocabulary list.

```{r bigram}
bigram <- corpus %>% 
        unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
        separate(ngram, c("word1", "word2"), sep = " ") %>%
        count(word1, word2) %>%
        filter(n > quantile(n,0.4)) %>%
        na.omit()
       
```

## Trigrams

Likewise, the trigrams and higher level n-grams are created in a similar manner.

```{r trigrams}
trigram <- corpus %>% 
        unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
        separate(ngram, c("word1", "word2", "word3"), sep = " ") %>%
        count(word1, word2, word3) %>%
        filter(n > quantile(n,0.4)) %>%
        na.omit()
rm(corpus)
```

## Modeling

The creation of a corpus is just the start. Models need to be created in order to do the actually prediction work. The main idea is to match the previous set of words with the words that make up the ngrams. The final word in the ngram is than the prediction. The prediction takes the form of a distribution as there are likely multiple matches in the corpus. Different models change the distribution and thus the probability of a word becoming the predicted word. This distribution is created by taking a count of all the matches in the corpus. Each count is than normalized by the total count of every possible match to produce a probabilty that adds to one. 

### Maximum Likelihood  Estimate

$$MLE = \frac{N_r}{N}$$

The Maximum Likelihood Estimate (MLE) is the simplest algorithm for examining the distribution. It simply returns the value with the highest count/occurrences in the corpus. $N_r$ represents the counts of a given word and $N$ represents that count for every word. This application can be extended to higher level ngrams, where previously typed words match the words in an ngram prior to the last value. The last value is than the predicted value. 

There are some issues with this method as for many words in the Vocabulary there are no occurrences and therefore no probability of the word being estimated. This is undesirable as we would like some probability assigned with every possible word.

### Add One Smoothing

$$AddOne = \frac{N_r+1}{N+len(Vocab)}$$

Add one smoothing (leplace smoothing) is a simple method to even out the distribution. Prior to calculating the MLE, each word in the vocabulary is given an extra count.

### Good-Turing

Good-Turing smoothing attempts to leverage the probability on lower ngram probabilities rather then fixed values. For a bigram model, the ngram probability (MLE) can provide more insight. 

This introduce the notation of $N_C$  which represents the count of occurrences with the frequency of $C$. The probability of things that have never occurred are defined based on the probability of things that have occurred once. 

$$P^*_{GT}(N_0) = \frac{N_1}{N}$$

The newly modified counts are defined as the following:

$$c^* = \frac{(c+1)N_{c+1}}{N_c}$$
The adjusted count, $c^*$, is modified by the frequency of the of the actual count +1 occurring over the frequency of the count occurring. This does create an issue when there are gaps in the frequencies of counts, as suggested by the following plot:

```{r freq, include=FALSE, eval=TRUE}
library(tidyverse)
df <- data.frame('Counts' = 1:9, 'Frequency' = c(100,50,42,30,15,10,4,0,1))
g <- df %>%
        ggplot(aes(x = Counts, y= Frequency)) +
        geom_bar(stat = 'identity')
```
```{r examplot, eval=TRUE, echo=FALSE}
plot(g)
```
From the plot, we can see that there is never an occurrence of a count of 8, but that there is one occurrence of a count of 9. To solve this issue, we can create regression model to fit over the frequency counts. Than, whenever the term $N_{c+1}$ is zero, the value can be replaced by the regression model. This model is generally a power model, fitting the log of the counts against the log of the frequencies, as described in the following plot:
```{r freqmdl, eval=TRUE, echo=FALSE}
g + geom_smooth(formula = y~x, data = df[is.finite(log(df$Frequency)),])
```

Through this method, any count over zero is decreased as the term $\frac{N_{c+1}}{N_c}$ is usually less than one as the count of ngrams decrease with the frequency of the count. This probability is transfer to ngrams with counts of zero.

```{r discounts, eval=TRUE, echo=FALSE}
library(gt)
data.frame('Count c' = 0:9, 'Good Turing c*' = c(0.00270, 0.446, 1.26, 2.24, 3.24, 4.22, 4.19, 6.21, 7.24, 8.25)) %>% gt()

```

### Absolute Discounting Interpolation


## Application

```{r app, warning=FALSE, echo=FALSE, eval=TRUE}
knitr::include_url(url = " https://m2edney.shinyapps.io/Text_Predictive_Model/?_ga=2.105454339.129590181.1646962570-1341333380.1645206372")
```

> Photo by [Sandy Millar](https://unsplash.com/@sandym10?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/predictive-text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
