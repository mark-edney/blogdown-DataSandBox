---
title: Text Prediction App
author: Mark Edney
date: '2022-03-11'
slug: []
categories:
  - Project
tags:
  - Shiny App
  - NLP
  - R
draft: yes
description: 'A predictive Text Shiny Application'
image: "/img/predict_text.jpg"
archives:
  - 2022/03
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<blockquote>
<p>This Shiny App was first written in May of 2021</p>
</blockquote>
<div id="description" class="section level2">
<h2>Description</h2>
<p>The goal of this project was to create a N-gram based model to predict the word to follow the users input. This project was to complete the Capstone project for the Johns Hopkins University Data science program on coursera. The data for this project was provided by swiftkey.</p>
</div>
<div id="initialization" class="section level2">
<h2>Initialization</h2>
<p>The initial step that loads the required libraries and downloads the data sets if not all read on file.</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(pryr)

#downloads the corpus files, profanity filter and English dictionary

url &lt;- &quot;https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip&quot;
url2 &lt;- &quot;https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip&quot;
url3 &lt;- &quot;https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt&quot;
url4 &lt;- &quot;https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt&quot;
if(dir.exists(&quot;~/R/Capestone/data/&quot;) == FALSE){
       dir.create(&quot;~/R/Capestone/data/&quot;)}

if(file.exists(&quot;~/R/Capestone/data/data.zip&quot;) == FALSE|
   file.exists(&quot;~/R/Capestone/data/prof.zip&quot;)==FALSE|
   file.exists(&quot;~/R/Capestone/data/diction.txt&quot;)==FALSE|
    file.exists(&quot;~/R/Capestone/data/contractions.txt&quot;)==FALSE){
        download.file(url,destfile = &quot;~/R/Capestone/data/data.zip&quot;)
        download.file(url2,destfile = &quot;~/R/Capestone/data/prof.zip&quot;)
        download.file(url3,destfile = &quot;~/R/Capestone/data/diction.txt&quot;)
        download.file(url4,destfile = &quot;~/R/Capestone/data/contractions.txt&quot;)
        setwd(&quot;~/R/Capestone/data/&quot;)
        unzip(&quot;~/R/Capestone/data/prof.zip&quot;)
        unzip(&quot;~/R/Capestone/data/data.zip&quot;)
        setwd(&quot;~/R/Capestone&quot;)
}</code></pre>
</div>
<div id="creating-a-corpus" class="section level2">
<h2>Creating a Corpus</h2>
<p>The project requires a Corpus, or a large body of text to create models. At this stage, the files are opened and joined. The Corpus is so large and requires some much ram that a sample of 10% is taken.</p>
<pre class="r"><code>blog &lt;- read_lines(&quot;~/R/Capestone/data/final/en_US/en_US.blogs.txt&quot;)
news &lt;- read_lines(&quot;~/R/Capestone/data/final/en_US/en_US.news.txt&quot;)
twitter &lt;- read_lines(&quot;~/R/Capestone/data/final/en_US/en_US.twitter.txt&quot;)
blog &lt;- tibble(text = blog) 
news &lt;- tibble(text = news)
twitter &lt;- tibble(text = twitter)

set.seed(90210)
corpus &lt;- bind_rows(blog,twitter,news) %&gt;% 
        slice_sample(prop = 0.10) %&gt;%
        mutate(line = row_number())</code></pre>
</div>
<div id="corpus-filtering" class="section level2">
<h2>Corpus filtering</h2>
<p>Here the corpus filter is created to remove profanity and any word that is not in the English dictionary.</p>
<pre class="r"><code>prof &lt;- read_lines(&quot;~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt&quot;)[15]
prof &lt;- prof %&gt;% str_split(&quot;, &quot;) %&gt;% flatten %&gt;% unlist
prof &lt;- tibble(&quot;word&quot; = prof)

english &lt;- read_lines(&quot;~/R/Capestone/data/diction.txt&quot;)
english &lt;- tibble(&quot;word&quot; = english[!english==&quot;&quot;])

contract &lt;- read_lines(&quot;~/R/Capestone/data/contractions.txt&quot;)
contract &lt;- tibble(&quot;word&quot; = contract) %&gt;% unnest_tokens(word,word)</code></pre>
</div>
<div id="vocabulary" class="section level2">
<h2>Vocabulary</h2>
<p>A vocabulary of words is created from the unique words with the applied filters.</p>
<pre class="r"><code>#clean up ram
rm(blog,news,twitter)
voc &lt;- bind_rows(english, contract) %&gt;% anti_join(prof)

unigram &lt;- corpus %&gt;% unnest_tokens(ngram, text, token = &quot;ngrams&quot;, n = 1) %&gt;%
        semi_join(voc, by = c(&quot;ngram&quot;=&quot;word&quot;)) 
#decreases the voc size
voc &lt;- tibble(word = unique(unigram$ngram))</code></pre>
</div>
<div id="out-of-vocabulary" class="section level2">
<h2>Out of Vocabulary</h2>
<p>To model out of vocabulary words we take a sampling of the least frequent unigrams and change them to the character “<unk>”. If a word is tested that isn’t in the vocabulary for the corpus, the quantity will be converted to “<unk>”.</p>
<pre class="r"><code>#OOV 1% of the least likely unigrams
unigramcount &lt;- unigram %&gt;% count(ngram)
unk &lt;- unigramcount %&gt;%
        filter(n==1) %&gt;%
        slice_sample(n = mean(unigramcount$n)) 
unigram[unigram$ngram %in% unk$ngram,]$ngram &lt;- &quot;unk&quot;</code></pre>
<p>The Corpus can reconstructed with the reduced vocabulary remove the need to re-filter for the higher levels of n-grams.</p>
<pre class="r"><code>corpus &lt;- unigram %&gt;%
        group_by(line) %&gt;%
        summarise(line = paste(ngram, collapse = &quot; &quot;)) %&gt;%
        rename(&quot;text&quot; = &quot;line&quot;) %&gt;%
        mutate(&quot;line&quot; = row_number())

rm(unigramcount)        </code></pre>
<p>There are still many word that only appear once so it useful to remove these words as the add little to prediction value of the models but significantly to the time and memory requirement. This is mainly due to the filtering.</p>
<pre class="r"><code>unigram &lt;- unigram %&gt;%
        count(ngram) %&gt;%
        filter(n &gt; quantile(n,0.4))

voc &lt;- tibble(word = unigram$ngram)</code></pre>
</div>
<div id="bigrams" class="section level2">
<h2>Bigrams</h2>
<p>The bigrams are created and then split into individual words which can be filtered by the vocabulary list.</p>
<pre class="r"><code>bigram &lt;- corpus %&gt;% 
        unnest_tokens(ngram, text, token = &quot;ngrams&quot;, n = 2) %&gt;%
        separate(ngram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;%
        count(word1, word2) %&gt;%
        filter(n &gt; quantile(n,0.4)) %&gt;%
        na.omit()</code></pre>
</div>
<div id="trigrams" class="section level2">
<h2>Trigrams</h2>
<p>Likewise, the trigrams and higher level n-grams are created in a similar manner.</p>
<pre class="r"><code>trigram &lt;- corpus %&gt;% 
        unnest_tokens(ngram, text, token = &quot;ngrams&quot;, n = 3) %&gt;%
        separate(ngram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;%
        count(word1, word2, word3) %&gt;%
        filter(n &gt; quantile(n,0.4)) %&gt;%
        na.omit()
rm(corpus)</code></pre>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
</div>
<div id="application" class="section level2">
<h2>Application</h2>
<iframe src=" https://m2edney.shinyapps.io/Text_Predictive_Model/?_ga=2.105454339.129590181.1646962570-1341333380.1645206372" width="672" height="400px" data-external="1">
</iframe>
<blockquote>
<p>Photo by <a href="https://unsplash.com/@sandym10?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Sandy Millar</a> on <a href="https://unsplash.com/s/photos/predictive-text?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p>
</blockquote>
</div>
