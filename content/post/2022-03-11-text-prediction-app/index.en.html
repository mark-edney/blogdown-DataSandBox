---
title: Text Prediction App
author: Mark Edney
date: '2022-03-11'
slug: []
categories:
  - Project
tags:
  - Shiny App
  - NLP
  - R
draft: yes
description: 'A predictive Text Shiny Application'
image: "/img/predict_text.jpg"
archives:
  - 2022/03
latex: true
---



<blockquote>
<p>This Shiny App was first written in May of 2021</p>
</blockquote>
<div id="description" class="section level2">
<h2>Description</h2>
<p>The goal of this project was to create a N-gram based model to predict the word to follow the users input. This project was to complete the Capstone project for the Johns Hopkins University Data science program on coursera. The data for this project was provided by swiftkey.</p>
</div>
<div id="initialization" class="section level2">
<h2>Initialization</h2>
<p>The initial step that loads the required libraries and downloads the data sets if not all read on file.</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(pryr)

#downloads the corpus files, profanity filter and English dictionary

url &lt;- &quot;https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip&quot;
url2 &lt;- &quot;https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip&quot;
url3 &lt;- &quot;https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt&quot;
url4 &lt;- &quot;https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt&quot;
if(dir.exists(&quot;~/R/Capestone/data/&quot;) == FALSE){
       dir.create(&quot;~/R/Capestone/data/&quot;)}

if(file.exists(&quot;~/R/Capestone/data/data.zip&quot;) == FALSE|
   file.exists(&quot;~/R/Capestone/data/prof.zip&quot;)==FALSE|
   file.exists(&quot;~/R/Capestone/data/diction.txt&quot;)==FALSE|
    file.exists(&quot;~/R/Capestone/data/contractions.txt&quot;)==FALSE){
        download.file(url,destfile = &quot;~/R/Capestone/data/data.zip&quot;)
        download.file(url2,destfile = &quot;~/R/Capestone/data/prof.zip&quot;)
        download.file(url3,destfile = &quot;~/R/Capestone/data/diction.txt&quot;)
        download.file(url4,destfile = &quot;~/R/Capestone/data/contractions.txt&quot;)
        setwd(&quot;~/R/Capestone/data/&quot;)
        unzip(&quot;~/R/Capestone/data/prof.zip&quot;)
        unzip(&quot;~/R/Capestone/data/data.zip&quot;)
        setwd(&quot;~/R/Capestone&quot;)
}</code></pre>
</div>
<div id="creating-a-corpus" class="section level2">
<h2>Creating a Corpus</h2>
<p>The project requires a Corpus, or a large body of text to create models. At this stage, the files are opened and joined. The Corpus is so large and requires some much ram that a sample of 10% is taken.</p>
<pre class="r"><code>blog &lt;- read_lines(&quot;~/R/Capestone/data/final/en_US/en_US.blogs.txt&quot;)
news &lt;- read_lines(&quot;~/R/Capestone/data/final/en_US/en_US.news.txt&quot;)
twitter &lt;- read_lines(&quot;~/R/Capestone/data/final/en_US/en_US.twitter.txt&quot;)
blog &lt;- tibble(text = blog) 
news &lt;- tibble(text = news)
twitter &lt;- tibble(text = twitter)

set.seed(90210)
corpus &lt;- bind_rows(blog,twitter,news) %&gt;% 
        slice_sample(prop = 0.10) %&gt;%
        mutate(line = row_number())</code></pre>
</div>
<div id="corpus-filtering" class="section level2">
<h2>Corpus filtering</h2>
<p>Here the corpus filter is created to remove profanity and any word that is not in the English dictionary.</p>
<pre class="r"><code>prof &lt;- read_lines(&quot;~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt&quot;)[15]
prof &lt;- prof %&gt;% str_split(&quot;, &quot;) %&gt;% flatten %&gt;% unlist
prof &lt;- tibble(&quot;word&quot; = prof)

english &lt;- read_lines(&quot;~/R/Capestone/data/diction.txt&quot;)
english &lt;- tibble(&quot;word&quot; = english[!english==&quot;&quot;])

contract &lt;- read_lines(&quot;~/R/Capestone/data/contractions.txt&quot;)
contract &lt;- tibble(&quot;word&quot; = contract) %&gt;% unnest_tokens(word,word)</code></pre>
</div>
<div id="vocabulary" class="section level2">
<h2>Vocabulary</h2>
<p>A vocabulary of words is created from the unique words with the applied filters.</p>
<pre class="r"><code>#clean up ram
rm(blog,news,twitter)
voc &lt;- bind_rows(english, contract) %&gt;% anti_join(prof)

unigram &lt;- corpus %&gt;% unnest_tokens(ngram, text, token = &quot;ngrams&quot;, n = 1) %&gt;%
        semi_join(voc, by = c(&quot;ngram&quot;=&quot;word&quot;)) 
#decreases the voc size
voc &lt;- tibble(word = unique(unigram$ngram))</code></pre>
</div>
<div id="out-of-vocabulary" class="section level2">
<h2>Out of Vocabulary</h2>
<p>To model out of vocabulary words we take a sampling of the least frequent unigrams and change them to the character “<unk>”. If a word is tested that isn’t in the vocabulary for the corpus, the quantity will be converted to “<unk>”.</p>
<pre class="r"><code>#OOV 1% of the least likely unigrams
unigramcount &lt;- unigram %&gt;% count(ngram)
unk &lt;- unigramcount %&gt;%
        filter(n==1) %&gt;%
        slice_sample(n = mean(unigramcount$n)) 
unigram[unigram$ngram %in% unk$ngram,]$ngram &lt;- &quot;unk&quot;</code></pre>
<p>The Corpus can reconstructed with the reduced vocabulary remove the need to re-filter for the higher levels of n-grams.</p>
<pre class="r"><code>corpus &lt;- unigram %&gt;%
        group_by(line) %&gt;%
        summarise(line = paste(ngram, collapse = &quot; &quot;)) %&gt;%
        rename(&quot;text&quot; = &quot;line&quot;) %&gt;%
        mutate(&quot;line&quot; = row_number())

rm(unigramcount)        </code></pre>
<p>There are still many words that only appear once so it useful to remove these words as the add little to prediction value of the models but significantly to the time and memory requirement. This is mainly due to the filtering.</p>
<pre class="r"><code>unigram &lt;- unigram %&gt;%
        count(ngram) %&gt;%
        filter(n &gt; quantile(n,0.4))

voc &lt;- tibble(word = unigram$ngram)</code></pre>
</div>
<div id="bigrams" class="section level2">
<h2>Bigrams</h2>
<p>The bigrams are created and then split into individual words which can be filtered by the vocabulary list.</p>
<pre class="r"><code>bigram &lt;- corpus %&gt;% 
        unnest_tokens(ngram, text, token = &quot;ngrams&quot;, n = 2) %&gt;%
        separate(ngram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;%
        count(word1, word2) %&gt;%
        filter(n &gt; quantile(n,0.4)) %&gt;%
        na.omit()</code></pre>
</div>
<div id="trigrams" class="section level2">
<h2>Trigrams</h2>
<p>Likewise, the trigrams and higher level n-grams are created in a similar manner.</p>
<pre class="r"><code>trigram &lt;- corpus %&gt;% 
        unnest_tokens(ngram, text, token = &quot;ngrams&quot;, n = 3) %&gt;%
        separate(ngram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;%
        count(word1, word2, word3) %&gt;%
        filter(n &gt; quantile(n,0.4)) %&gt;%
        na.omit()
rm(corpus)</code></pre>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<p>The creation of a corpus is just the start. Models need to be created in order to do the actually prediction work. The main idea is to match the previous set of words with the words that make up the ngrams. The final word in the ngram is than the prediction. The prediction takes the form of a distribution as there are likely multiple matches in the corpus. Different models change the distribution and thus the probability of a word becoming the predicted word. This distribution is created by taking a count of all the matches in the corpus. Each count is than normalized by the total count of every possible match to produce a probabilty that adds to one.</p>
<div id="maximum-likelihood-estimate" class="section level3">
<h3>Maximum Likelihood Estimate</h3>
<p><span class="math display">\[MLE = \frac{N_r}{N}\]</span></p>
<p>The Maximum Likelihood Estimate (MLE) is the simplest algorithm for examining the distribution. It simply returns the value with the highest count/occurrences in the corpus. <span class="math inline">\(N_r\)</span> represents the counts of a given word and <span class="math inline">\(N\)</span> represents that count for every word. This application can be extended to higher level ngrams, where previously typed words match the words in an ngram prior to the last value. The last value is than the predicted value.</p>
<p>There are some issues with this method as for many words in the Vocabulary there are no occurrences and therefore no probability of the word being estimated. This is undesirable as we would like some probability assigned with every possible word.</p>
</div>
<div id="add-one-smoothing" class="section level3">
<h3>Add One Smoothing</h3>
<p><span class="math display">\[AddOne = \frac{N_r+1}{N+len(Vocab)}\]</span></p>
<p>Add one smoothing (leplace smoothing) is a simple method to even out the distribution. Prior to calculating the MLE, each word in the vocabulary is given an extra count.</p>
</div>
<div id="good-turing" class="section level3">
<h3>Good-Turing</h3>
<p>Good-Turing smoothing attempts to leverage the probability on lower ngram probabilities rather then fixed values. For a bigram model, the ngram probability (MLE) can provide more insight.</p>
<p>This introduce the notation of <span class="math inline">\(N_C\)</span> which represents the count of occurrences with the frequency of <span class="math inline">\(C\)</span>. The probability of things that have never occurred are defined based on the probability of things that have occurred once.</p>
<p><span class="math display">\[P^*_{GT}(N_0) = \frac{N_1}{N}\]</span></p>
<p>The newly modified counts are defined as the following:</p>
<p><span class="math display">\[c^* = \frac{(c+1)N_{c+1}}{N_c}\]</span>
The adjusted count, <span class="math inline">\(c^*\)</span>, is modified by the frequency of the of the actual count +1 occurring over the frequency of the count occurring. This does create an issue when there are gaps in the frequencies of counts, as suggested by the following plot:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/examplot-1.png" width="672" />
From the plot, we can see that there is never an occurrence of a count of 8, but that there is one occurrence of a count of 9. To solve this issue, we can create regression model to fit over the frequency counts. Than, whenever the term <span class="math inline">\(N_{c+1}\)</span> is zero, the value can be replaced by the regression model. This model is generally a power model, fitting the log of the counts against the log of the frequencies, as described in the following plot:
<img src="{{< blogdown/postref >}}index.en_files/figure-html/freqmdl-1.png" width="672" /></p>
<p>Through this method, any count over zero is decreased as the term <span class="math inline">\(\frac{N_{c+1}}{N_c}\)</span> is usually less than one as the count of ngrams decrease with the frequency of the count. This probability is transfer to ngrams with counts of zero.</p>
<div id="nsjvjuhugr" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#nsjvjuhugr .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nsjvjuhugr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nsjvjuhugr .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nsjvjuhugr .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nsjvjuhugr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nsjvjuhugr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nsjvjuhugr .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nsjvjuhugr .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nsjvjuhugr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nsjvjuhugr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nsjvjuhugr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nsjvjuhugr .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#nsjvjuhugr .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nsjvjuhugr .gt_from_md > :first-child {
  margin-top: 0;
}

#nsjvjuhugr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#nsjvjuhugr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nsjvjuhugr .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#nsjvjuhugr .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#nsjvjuhugr .gt_row_group_first td {
  border-top-width: 2px;
}

#nsjvjuhugr .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nsjvjuhugr .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#nsjvjuhugr .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#nsjvjuhugr .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nsjvjuhugr .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nsjvjuhugr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nsjvjuhugr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nsjvjuhugr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nsjvjuhugr .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nsjvjuhugr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#nsjvjuhugr .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nsjvjuhugr .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#nsjvjuhugr .gt_left {
  text-align: left;
}

#nsjvjuhugr .gt_center {
  text-align: center;
}

#nsjvjuhugr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nsjvjuhugr .gt_font_normal {
  font-weight: normal;
}

#nsjvjuhugr .gt_font_bold {
  font-weight: bold;
}

#nsjvjuhugr .gt_font_italic {
  font-style: italic;
}

#nsjvjuhugr .gt_super {
  font-size: 65%;
}

#nsjvjuhugr .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

#nsjvjuhugr .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#nsjvjuhugr .gt_slash_mark {
  font-size: 0.7em;
  line-height: 0.7em;
  vertical-align: 0.15em;
}

#nsjvjuhugr .gt_fraction_numerator {
  font-size: 0.6em;
  line-height: 0.6em;
  vertical-align: 0.45em;
}

#nsjvjuhugr .gt_fraction_denominator {
  font-size: 0.6em;
  line-height: 0.6em;
  vertical-align: -0.05em;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">Count.c</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">Good.Turing.c.</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_right">0</td>
<td class="gt_row gt_right">0.0027</td></tr>
    <tr><td class="gt_row gt_right">1</td>
<td class="gt_row gt_right">0.4460</td></tr>
    <tr><td class="gt_row gt_right">2</td>
<td class="gt_row gt_right">1.2600</td></tr>
    <tr><td class="gt_row gt_right">3</td>
<td class="gt_row gt_right">2.2400</td></tr>
    <tr><td class="gt_row gt_right">4</td>
<td class="gt_row gt_right">3.2400</td></tr>
    <tr><td class="gt_row gt_right">5</td>
<td class="gt_row gt_right">4.2200</td></tr>
    <tr><td class="gt_row gt_right">6</td>
<td class="gt_row gt_right">4.1900</td></tr>
    <tr><td class="gt_row gt_right">7</td>
<td class="gt_row gt_right">6.2100</td></tr>
    <tr><td class="gt_row gt_right">8</td>
<td class="gt_row gt_right">7.2400</td></tr>
    <tr><td class="gt_row gt_right">9</td>
<td class="gt_row gt_right">8.2500</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
<div id="absolute-discounting-interpolation" class="section level3">
<h3>Absolute Discounting Interpolation</h3>
</div>
</div>
<div id="application" class="section level2">
<h2>Application</h2>
<iframe src=" https://m2edney.shinyapps.io/Text_Predictive_Model/?_ga=2.105454339.129590181.1646962570-1341333380.1645206372" width="672" height="400px" data-external="1">
</iframe>
<blockquote>
<p>Photo by <a href="https://unsplash.com/@sandym10?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Sandy Millar</a> on <a href="https://unsplash.com/s/photos/predictive-text?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p>
</blockquote>
</div>
